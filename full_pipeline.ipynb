{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68e77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1e494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c6d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations: 13320\n",
      "Splits available: ['train1', 'train2', 'train3', 'test1', 'test2', 'test3']\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/ucf101_2d.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "splits = data['split']\n",
    "annotations = data['annotations']\n",
    "\n",
    "print(f\"Total annotations: {len(annotations)}\")\n",
    "print(f\"Splits available: {list(splits.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2cb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile of frame counts: 359\n"
     ]
    }
   ],
   "source": [
    "frame_counts =[]\n",
    "num_persons = []\n",
    "keypoint_shapes = []\n",
    "\n",
    "for ann in annotations:\n",
    "    keypoints = ann['keypoint']\n",
    "    frame_counts.append(ann['total_frames'])\n",
    "    num_persons.append(keypoints.shape[0])\n",
    "    keypoint_shapes.append(keypoints.shape)\n",
    "\n",
    "frame_counts = np.array(frame_counts) \n",
    "num_persons = np.array(num_persons)\n",
    "\n",
    "MAX_FRAMES = int(np.percentile(frame_counts, 95))\n",
    "print(f\"95th percentile of frame counts: {MAX_FRAMES}\")\n",
    "#Hecho así para evitar videos extremadamente largos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c997e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF101Dataset(Dataset):\n",
    "    def __init__(self, annotations, indices, max_frames=119, person_strategy='first', augment=False):\n",
    "        self.annotations = [annotations[i] for i in indices]\n",
    "        self.max_frames = max_frames\n",
    "        self.person_strategy = person_strategy\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def _select_person(self, keypoints, keypoint_scores):\n",
    "        \n",
    "        num_persons = keypoints.shape[0]\n",
    "        if num_persons == 1:\n",
    "            return keypoints[0], keypoint_scores[0]\n",
    "        \n",
    "        if self.person_strategy == 'first':\n",
    "            return keypoints[0], keypoint_scores[0]\n",
    "        \n",
    "        elif self.person_strategy == 'max_conf':\n",
    "            avg_conf_per_person = keypoint_scores.mean(axis=(1, 2))\n",
    "            best_person_idx = avg_conf_per_person.argmax()\n",
    "            return keypoints[best_person_idx], keypoint_scores[best_person_idx]\n",
    "        \n",
    "        elif self.person_strategy == 'average':\n",
    "            \n",
    "            weights = keypoint_scores[..., np.newaxis]  # (num_persons, T, 17, 1)\n",
    "            weighted_keypoints = (keypoints * weights).sum(axis=0)\n",
    "            total_weights = weights.sum(axis=0) + 1e-8\n",
    "            avg_keypoints = weighted_keypoints / total_weights\n",
    "            avg_scores = keypoint_scores.mean(axis=0)\n",
    "            return avg_keypoints, avg_scores\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Unknown person selection strategy: {self.person_strategy}\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.annotations[idx]\n",
    "\n",
    "        keypoints = sample['keypoint']  \n",
    "        keypoint_scores = sample['keypoint_score']\n",
    "        label = sample['label']\n",
    "\n",
    "        keypoints, keypoint_scores = self._select_person(keypoints, keypoint_scores)\n",
    "        \n",
    "        T, num_joints, coords = keypoints.shape\n",
    "\n",
    "        if T < self.max_frames:\n",
    "            pad_frames = self.max_frames - T\n",
    "            keypoints = np.pad(keypoints, ((0, pad_frames), (0, 0), (0, 0)), mode='constant')\n",
    "            keypoint_scores = np.pad(keypoint_scores, ((0, pad_frames), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            keypoints = keypoints[:self.max_frames]\n",
    "            keypoint_scores = keypoint_scores[:self.max_frames]\n",
    "\n",
    "        features = np.concatenate([keypoints, keypoint_scores[..., np.newaxis]], axis=-1)\n",
    "\n",
    "        if self.augment:\n",
    "            noise = np.random.randn(*features.shape) * 0.01\n",
    "            features = features + noise\n",
    "\n",
    "        features = features.transpose(2, 0, 1)\n",
    "\n",
    "        return torch.FloatTensor(features), torch.LongTensor([label])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e606c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseKeypointCNN(nn.Module):\n",
    "    def __init__(self, num_classes=101, max_frames=119, num_joints=17, in_channels=3):\n",
    "        super(PoseKeypointCNN, self).__init__()\n",
    "        \n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=(5, 1), padding=(2, 0)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(5, 1), padding=(2, 0)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=(5, 1), padding=(2, 0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.spatial_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.spatial_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdf40bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m train_indices = splits[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPLIT_NUM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m test_indices = splits[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPLIT_NUM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_dataset = \u001b[43mUCF101Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mperson_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPERSON_STRATEGY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m test_dataset = UCF101Dataset(annotations, test_indices, MAX_FRAMES, \n\u001b[32m     15\u001b[39m                                      person_strategy=PERSON_STRATEGY, augment=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     17\u001b[39m train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mUCF101Dataset.__init__\u001b[39m\u001b[34m(self, annotations, indices, max_frames, person_strategy, augment)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotations, indices, max_frames=\u001b[32m119\u001b[39m, person_strategy=\u001b[33m'\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m'\u001b[39m, augment=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.annotations = [\u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_frames = max_frames\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.person_strategy = person_strategy\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "SPLIT_NUM = 1\n",
    "\n",
    "PERSON_STRATEGY = 'max_conf'  # Options: 'first', 'max_conf', 'average'\n",
    "\n",
    "train_indices = splits[f'train{SPLIT_NUM}']\n",
    "test_indices = splits[f'test{SPLIT_NUM}']\n",
    "\n",
    "train_dataset = UCF101Dataset(annotations, train_indices, MAX_FRAMES, \n",
    "                                      person_strategy=PERSON_STRATEGY, augment=True)\n",
    "test_dataset = UCF101Dataset(annotations, test_indices, MAX_FRAMES, \n",
    "                                     person_strategy=PERSON_STRATEGY, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "num_classes = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoseKeypointCNN(num_classes=num_classes, max_frames=MAX_FRAMES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for features, labels in tqdm(loader, desc=\"Training\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f'best_model_split{SPLIT_NUM}.pth')\n",
    "        print(f\"✓ Saved best model (Val Acc: {val_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7267f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'best_model_split{SPLIT_NUM}.pth'))\n",
    "_, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(train_accs, label='Train Acc')\n",
    "axes[1].plot(val_accs, label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'training_curves_split{SPLIT_NUM}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Training complete! Best model saved as 'best_model_split{SPLIT_NUM}.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
